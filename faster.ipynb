{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2669292710015725\n",
      "1.2488546700005827\n",
      "0.28119098099887196\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from timeit import timeit\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "csc_codes = load_npz(\"csc_codes.npz\")\n",
    "\n",
    "time = lambda fn, number=100: print(timeit(fn, number=number))\n",
    "\n",
    "def get_topk(arr, k=10):\n",
    "    assert isinstance(k, int) and k > 0\n",
    "    if not isinstance(arr, np.ndarray):\n",
    "        arr = np.array(arr)\n",
    "    assert len(arr.shape) == 1\n",
    "\n",
    "    indices = arr.argsort()[-k:][::-1]\n",
    "    vals = arr[indices]\n",
    "    return indices.tolist(), vals.tolist()\n",
    "\n",
    "\n",
    "\n",
    "def baseline():\n",
    "    idx = np.random.randint(0, csc_codes.shape[1])\n",
    "    get_topk(arr=csc_codes[:,idx].toarray().flatten(), k=400)\n",
    "# time(lambda: csc_codes[:,1].todense().flatten(), number=1000)\n",
    "# time(lambda: topk(x, k=100), number=1000)\n",
    "# atom_result = atom_query(atom_idx, csc_codes, k=k, lowest_ratio=lowest_ratio, string=True)\n",
    "# toks, weights = topk(csc_codes[:, atom_idx].toarray().flatten(), k=k)\n",
    "\n",
    "\n",
    "# test_arr = np.random.rand(10000)\n",
    "\n",
    "# # timeit\n",
    "# print(timeit.timeit(lambda: topk(test_arr), number=10000))\n",
    "\n",
    "def get_top_k_indices(arr, k):\n",
    "    if arr.shape[0] < k:\n",
    "        partitioned_indices = np.arange(arr.shape[0])\n",
    "    else:\n",
    "        partitioned_indices = np.argpartition(arr, -k)[-k:]\n",
    "    return partitioned_indices[np.argsort(-arr[partitioned_indices])]\n",
    "\n",
    "def topk_optimized(arr, k=10):\n",
    "    if not isinstance(arr, np.ndarray):\n",
    "        raise ValueError(\"Expected input to be a numpy array\")\n",
    "    \n",
    "    top_k_indices = get_top_k_indices(arr, k)\n",
    "    \n",
    "    return top_k_indices.tolist(), arr[top_k_indices].tolist()\n",
    "\n",
    "# # timeit    \n",
    "# print(timeit.timeit(lambda: topk_optimized(test_arr), number=10000))\n",
    "\n",
    "# topk(csc_codes[:,1].data, k=10)\n",
    "\n",
    "\n",
    "# import torch\n",
    "def experimental():\n",
    "    idx = np.random.randint(0, csc_codes.shape[1])\n",
    "    indices = csc_codes[:,idx].indices\n",
    "    _idx, vals = get_topk(csc_codes[:,idx].data, k=400)\n",
    "    tok_ids = indices[_idx]\n",
    "    return tok_ids, vals\n",
    "\n",
    "def experimental_two():\n",
    "    idx = np.random.randint(0, csc_codes.shape[1])\n",
    "    indices = csc_codes[:,idx].indices\n",
    "    _idx, vals = topk_optimized(csc_codes[:,idx].data, k=400)\n",
    "    tok_ids = indices[_idx]\n",
    "    return tok_ids, vals\n",
    "\n",
    "\n",
    "time(experimental, number=2000)\n",
    "time(baseline, number=2000)\n",
    "time(experimental_two, number=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.331315841000105\n",
      "1.7935730469998816\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from timeit import timeit\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "csc_codes = load_npz(\"csc_codes.npz\")\n",
    "csr_codes = load_npz(\"csr_codes.npz\")\n",
    "\n",
    "time = lambda fn, number=100: print(timeit(fn, number=number))\n",
    "\n",
    "def get_topk(arr, k=10):\n",
    "    assert isinstance(k, int) and k > 0\n",
    "    if not isinstance(arr, np.ndarray):\n",
    "        arr = np.array(arr)\n",
    "    assert len(arr.shape) == 1\n",
    "\n",
    "    indices = arr.argsort()[-k:][::-1]\n",
    "    vals = arr[indices]\n",
    "    return indices.tolist(), vals.tolist()\n",
    "\n",
    "\n",
    "\n",
    "# def baseline():\n",
    "#     idx = np.random.randint(0, csc_codes.shape[1])\n",
    "#     get_topk(arr=csc_codes[idx].toarray().flatten(), k=400)\n",
    "# # time(lambda: csc_codes[:,1].todense().flatten(), number=1000)\n",
    "# # time(lambda: topk(x, k=100), number=1000)\n",
    "# # atom_result = atom_query(atom_idx, csc_codes, k=k, lowest_ratio=lowest_ratio, string=True)\n",
    "# # toks, weights = topk(csc_codes[:, atom_idx].toarray().flatten(), k=k)\n",
    "\n",
    "\n",
    "\n",
    "def experimental():\n",
    "    idx = np.random.randint(0, csr_codes.shape[0])\n",
    "    indices = csr_codes[idx].indices\n",
    "    _idx, vals = get_topk(csr_codes[idx].data, k=400)\n",
    "    tok_ids = indices[_idx]\n",
    "    return tok_ids, vals\n",
    "\n",
    "# def experimental_two():\n",
    "#     idx = np.random.randint(0, csr_codes.shape[0])\n",
    "#     indices = csc_codes[idx].indices\n",
    "#     _idx, vals = get_topk(csc_codes[:,idx].data, k=400)\n",
    "#     tok_ids = indices[_idx]\n",
    "#     return tok_ids, vals\n",
    "\n",
    "def baseline():\n",
    "    idx = np.random.randint(0, csr_codes.shape[0])\n",
    "    get_topk(arr=csr_codes[idx].toarray().flatten(), k=400)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# def experimental_two():\n",
    "#     idx = np.random.randint(0, csc_codes.shape[1])\n",
    "#     indices = csc_codes[:,idx].indices\n",
    "#     _idx, vals = topk_optimized(csc_codes[:,idx].data, k=400)\n",
    "#     tok_ids = indices[_idx]\n",
    "#     return tok_ids, vals\n",
    "\n",
    "\n",
    "time(experimental, number=20000)\n",
    "time(baseline, number=20000)\n",
    "# time(experimental_two, number=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained('EleutherAI/pythia-70m-v0')\n",
    "\n",
    "tokens = [tokenizer.decode([tok_id]) for tok_id in range(50277)]\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"tokens.json\", \"w\") as f:\n",
    "    json.dump(tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50277"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50276])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([109,  45,   5,   5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0], dtype=torch.uint8),\n",
       "indices=tensor([ 176,  244,    2,   40, 1111, 1113, 1110, 1109, 1108, 1107, 1106, 1105,\n",
       "        1104, 1103, 1102, 1101, 1100, 1099, 1098, 1097, 1123, 1151, 1152, 1153,\n",
       "        1154, 1155, 1156, 1149, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144,\n",
       "        1145, 1146, 1147, 1205, 1112, 1136, 1122, 1121, 1120, 1119, 1118, 1117,\n",
       "        1116, 1115, 1114, 1159, 1192, 1181, 1182, 1183, 1184, 1185, 1186, 1187,\n",
       "        1188, 1189, 1190, 1191, 1180, 1193, 1194, 1195, 1196, 1197, 1198, 1199,\n",
       "        1200, 1201, 1202, 1203, 1169, 1158, 1148, 1160, 1161, 1162, 1163, 1164,\n",
       "        1165, 1166, 1167, 1168, 1157, 1170, 1171, 1172, 1173, 1174, 1175, 1176,\n",
       "        1177, 1178, 1179, 1128]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline\n",
    "import torch\n",
    "torch.tensor(csr_codes[123].toarray().flatten()).topk(k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  40 176 244]\n",
      "[  5   5 109  45]\n"
     ]
    }
   ],
   "source": [
    "idx = 123\n",
    "\n",
    "def experimental():\n",
    "    idx = np.random.randint(0, csr_codes.shape[0])\n",
    "    indices = csr_codes[idx].indices\n",
    "    _idx, vals = get_topk(csr_codes[:,idx].data, k=400)\n",
    "    tok_ids = indices[_idx]\n",
    "    return tok_ids, vals\n",
    "\n",
    "print(csr_codes[idx].indices)\n",
    "print(csr_codes[idx].data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thefuzz\n",
    "from thefuzz import process, fuzz\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"tokens.json\", \"r\") as f:\n",
    "    tokens = np.array(json.load(f))\n",
    "\n",
    "def update_scores(query, candidates, scores):\n",
    "    new_scores = [score+0.6*fuzz.ratio(query, c) for c, score in zip(candidates, scores)]\n",
    "    new_candidates = [c for new_score, c in sorted(zip(new_scores, candidates), reverse=True) if new_score >= 100 and abs(len(query) - len(c)) < 6 and len(c) > 3]\n",
    "    # all_candidates = [c for new_score, c in sorted(zip(new_scores, candidates), reverse=True)]\n",
    "    # print(all_candidates)\n",
    "    new_scores = sorted(new_scores, reverse=True)\n",
    "    return new_candidates, new_scores\n",
    "\n",
    "def process_candidates(query, candidates_and_scores):\n",
    "\n",
    "    filtered = [\n",
    "        (c, score + 0.4 * fuzz.ratio(query, c))\n",
    "        for c, score in candidates_and_scores\n",
    "        if score + 0.6 * fuzz.ratio(query, c) >= 60\n",
    "        and abs(len(query) - len(c)) < 6\n",
    "        and len(c) > 3\n",
    "    ]\n",
    "\n",
    "    new_candidates, new_scores = zip(*sorted(filtered, key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    return list(new_candidates), list(new_scores)\n",
    "\n",
    "\n",
    "flattened_tokens = [tok.strip().lower() for tok in tokens]\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def get_matches(query, k=10):\n",
    "    flattened_query = query.strip().lower()\n",
    "    flattened_query_len = len(flattened_query)\n",
    "\n",
    "    # Get topk using something like prefix matching on strings without spaces or capitalization\n",
    "    prefix_scores = [int(tok.startswith(flattened_query))*(flattened_query_len-0.2*len(tok))+int(flattened_query.startswith(tok))*(len(tok)-0.2*flattened_query_len) - 20*(tok == '') - (len(tok) < 3) for tok in flattened_tokens]\n",
    "    prefix_scores = np.array(prefix_scores)\n",
    "    topk_indices = prefix_scores.argpartition(-k)[-k:]\n",
    "    candidates, scores = tokens[topk_indices].tolist(), prefix_scores[topk_indices].tolist()\n",
    "\n",
    "    # Drop candidates with 0 score\n",
    "    candidates = [c for c, score in zip(candidates, scores) if score > 0]\n",
    "\n",
    "    # Reorder candidates to take into account capitalization/spacing\n",
    "    fuzz_scores = [score + fuzz.ratio(query, c)/100 for c, score in zip(candidates, scores)]\n",
    "    candidates = [c for fuzz_score, c in sorted(zip(fuzz_scores, candidates), reverse=True)]\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 3, 18]\n",
      "[0.57, 0.82, 0.43, 1.0]\n",
      "[3.57, 6.82, 3.43, 19.0]\n",
      "[' bri', ' brilli', ' Bri', ' brilliant']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' brilliant', ' brilli', ' bri', ' Bri']"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "\n",
    "from thefuzz import fuzz\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "def get_suggestions(query, k=5):\n",
    "    flattened_query = query.strip().lower()\n",
    "    flattened_query_len = len(flattened_query)\n",
    "\n",
    "    # Get topk using something like prefix matching on strings without spaces or capitalization\n",
    "    prefix_scores = [int(tok.startswith(flattened_query))*(flattened_query_len)+int(flattened_query.startswith(tok))*len(tok) - 1000*((tok == '')+(len(tok) < 3)) for tok in flattened_tokens]\n",
    "    prefix_scores = np.array(prefix_scores)\n",
    "    topk_indices = prefix_scores.argpartition(-k)[-k:]\n",
    "    candidates, scores = tokens[topk_indices].tolist(), prefix_scores[topk_indices].tolist()\n",
    "\n",
    "    # Drop candidates with 0 score\n",
    "    candidates = [c for c, score in zip(candidates, scores) if score > 0]\n",
    "    scores = [score for score in scores if score > 0]\n",
    "\n",
    "    # Reorder candidates to take into account capitalization/spacing\n",
    "    print(scores)\n",
    "    print([(fuzz.ratio(query, c)/100) for c, score in zip(candidates, scores)])\n",
    "    fuzz_scores = [score + (fuzz.ratio(query, c)/100) for c, score in zip(candidates, scores)]\n",
    "    print(fuzz_scores)\n",
    "    print(candidates)\n",
    "    candidates = [c for fuzz_score, c in sorted(zip(fuzz_scores, candidates), reverse=True)]\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "get_suggestions(' brilliant')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparse-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
